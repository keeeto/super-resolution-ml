<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>interim-report</title><link href='https://fonts.loli.net/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext' rel='stylesheet' type='text/css' /><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; --monospace:"Lucida Console",Consolas,"Courier",monospace; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; bottom: 0px; top: 0px; left: 0px; right: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; tab-size: 4; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 40px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 2; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px !important; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-diagram-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; }
  #write { margin-top: 0px; padding-top: 0px; border-color: transparent !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  html.blink-to-pdf { font-size: 13px; }
  .typora-export #write { padding-left: 32px; padding-right: 32px; padding-bottom: 0px; break-after: avoid; }
  .typora-export #write::after { height: 0px; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
[contenteditable="true"]:active, [contenteditable="true"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.8; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.md-inline-math .MathJax_SVG .noError { display: none !important; }
.html-for-mac .inline-math-svg .MathJax_SVG { vertical-align: 0.2px; }
.md-math-block .MathJax_SVG_Display { text-align: center; margin: 0px; position: relative; text-indent: 0px; max-width: none; max-height: none; min-height: 0px; min-width: 100%; width: auto; overflow-y: hidden; display: block !important; }
.MathJax_SVG_Display, .md-inline-math .MathJax_SVG_Display { width: auto; margin: inherit; display: inline-block !important; }
.MathJax_SVG .MJX-monospace { font-family: var(--monospace); }
.MathJax_SVG .MJX-sans-serif { font-family: sans-serif; }
.MathJax_SVG { display: inline; font-style: normal; font-weight: 400; line-height: normal; zoom: 90%; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; }
.MathJax_SVG * { transition: none 0s ease 0s; }
.MathJax_SVG_Display svg { vertical-align: middle !important; margin-bottom: 0px !important; margin-top: 0px !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="mermaid"] svg, [lang="flow"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }
svg[id^="mermaidChart"] { line-height: 1em; }
mark { background: rgb(255, 255, 0); color: rgb(0, 0, 0); }
.md-html-inline .md-plain, .md-html-inline strong, mark .md-inline-math, mark strong { color: inherit; }
mark .md-meta { color: rgb(0, 0, 0); opacity: 0.3 !important; }


:root {
    --side-bar-bg-color: #fafafa;
    --control-text-color: #777;
}

@include-when-export url(https://fonts.loli.net/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext);

html {
    font-size: 16px;
}

body {
    font-family: "Open Sans","Clear Sans","Helvetica Neue",Helvetica,Arial,sans-serif;
    color: rgb(51, 51, 51);
    line-height: 1.6;
}

#write {
    max-width: 860px;
  	margin: 0 auto;
  	padding: 30px;
    padding-bottom: 100px;
}
#write > ul:first-child,
#write > ol:first-child{
    margin-top: 30px;
}

a {
    color: #4183C4;
}
h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 1rem;
    font-weight: bold;
    line-height: 1.4;
    cursor: text;
}
h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}
h1 tt,
h1 code {
    font-size: inherit;
}
h2 tt,
h2 code {
    font-size: inherit;
}
h3 tt,
h3 code {
    font-size: inherit;
}
h4 tt,
h4 code {
    font-size: inherit;
}
h5 tt,
h5 code {
    font-size: inherit;
}
h6 tt,
h6 code {
    font-size: inherit;
}
h1 {
    padding-bottom: .3em;
    font-size: 2.25em;
    line-height: 1.2;
    border-bottom: 1px solid #eee;
}
h2 {
   padding-bottom: .3em;
    font-size: 1.75em;
    line-height: 1.225;
    border-bottom: 1px solid #eee;
}
h3 {
    font-size: 1.5em;
    line-height: 1.43;
}
h4 {
    font-size: 1.25em;
}
h5 {
    font-size: 1em;
}
h6 {
   font-size: 1em;
    color: #777;
}
p,
blockquote,
ul,
ol,
dl,
table{
    margin: 0.8em 0;
}
li>ol,
li>ul {
    margin: 0 0;
}
hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

li p.first {
    display: inline-block;
}
ul,
ol {
    padding-left: 30px;
}
ul:first-child,
ol:first-child {
    margin-top: 0;
}
ul:last-child,
ol:last-child {
    margin-bottom: 0;
}
blockquote {
    border-left: 4px solid #dfe2e5;
    padding: 0 15px;
    color: #777777;
}
blockquote blockquote {
    padding-right: 0;
}
table {
    padding: 0;
    word-break: initial;
}
table tr {
    border-top: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}
table tr:nth-child(2n),
thead {
    background-color: #f8f8f8;
}
table tr th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    margin: 0;
    padding: 6px 13px;
}
table tr td {
    border: 1px solid #dfe2e5;
    margin: 0;
    padding: 6px 13px;
}
table tr th:first-child,
table tr td:first-child {
    margin-top: 0;
}
table tr th:last-child,
table tr td:last-child {
    margin-bottom: 0;
}

.CodeMirror-lines {
    padding-left: 4px;
}

.code-tooltip {
    box-shadow: 0 1px 1px 0 rgba(0,28,36,.3);
    border-top: 1px solid #eef2f2;
}

.md-fences,
code,
tt {
    border: 1px solid #e7eaed;
    background-color: #f8f8f8;
    border-radius: 3px;
    padding: 0;
    padding: 2px 4px 0px 4px;
    font-size: 0.9em;
}

code {
    background-color: #f3f4f4;
    padding: 0 2px 0 2px;
}

.md-fences {
    margin-bottom: 15px;
    margin-top: 15px;
    padding-top: 8px;
    padding-bottom: 6px;
}


.md-task-list-item > input {
  margin-left: -1.3em;
}

@media print {
    html {
        font-size: 13px;
    }
    table,
    pre {
        page-break-inside: avoid;
    }
    pre {
        word-wrap: break-word;
    }
}

.md-fences {
	background-color: #f8f8f8;
}
#write pre.md-meta-block {
	padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block>.code-tooltip {
	bottom: .375rem;
}

.md-mathjax-midline {
    background: #fafafa;
}

#write>h3.md-focus:before{
	left: -1.5625rem;
	top: .375rem;
}
#write>h4.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h5.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h6.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
.md-image>.md-meta {
    /*border: 1px solid #ddd;*/
    border-radius: 3px;
    padding: 2px 0px 0px 4px;
    font-size: 0.9em;
    color: inherit;
}

.md-tag {
    color: #a7a7a7;
    opacity: 1;
}

.md-toc { 
    margin-top:20px;
    padding-bottom:20px;
}

.sidebar-tabs {
    border-bottom: none;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

/** focus mode */
.on-focus-mode blockquote {
    border-left-color: rgba(85, 85, 85, 0.12);
}

header, .context-menu, .megamenu-content, footer{
    font-family: "Segoe UI", "Arial", sans-serif;
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state{
    visibility: visible;
}

.mac-seamless-mode #typora-sidebar {
    background-color: #fafafa;
    background-color: var(--side-bar-bg-color);
}

.md-lang {
    color: #b4654d;
}

.html-for-mac .context-menu {
    --item-hover-bg-color: #E6F0FE;
}

#md-notification .btn {
    border: 0;
}

.dropdown-menu .divider {
    border-color: #e5e5e5;
}

.ty-preferences .window-content {
    background-color: #fafafa;
}

.ty-preferences .nav-group-item.active {
    color: white;
    background: #999;
}


</style>
</head>
<body class='typora-export os-windows' >
<div  id='write'  class = 'is-node'><ul><li><p><a href='#1-project-details'><span>1 Project Details</span></a></p></li><li><p><a href='#2-project-team'><span>2 Project Team</span></a></p><ul><li><a href='#21-principal-investigator'><span>2.1 Principal investigator</span></a></li><li><a href='#22-co-investigators'><span>2.2 Co-investigators</span></a></li></ul></li><li><p><a href='#3-publicity-summary'><span>3. Publicity summary</span></a></p></li><li><p><a href='#4-executive-summary'><span>4 Executive summary</span></a></p></li><li><p><a href='#5-aims-and-objectives'><span>5 Aims and objectives</span></a></p></li><li><p><a href='#6-methodology'><span>6 Methodology</span></a></p><ul><li><a href='#61-scientific-methodology'><span>6.1 Scientific methodology</span></a></li><li><a href='#62-ai-methodology'><span>6.2 AI methodology</span></a></li></ul></li><li><p><a href='#7-interim-results'><span>7 Interim results</span></a></p><ul><li><a href='#71-libraries'><span>7.1 Libraries</span></a></li><li><a href='#72-cnn-tomographic-reconstruction'><span>7.2 CNN tomographic reconstruction</span></a></li><li><a href='#73-msdn-for-reconstruction-super-resolution'><span>7.3 MSDN for reconstruction/super-resolution</span></a></li></ul></li><li><p><a href='#8-outputs'><span>8 Outputs</span></a></p><ul><li><a href='#81-libraries'><span>8.1 Libraries</span></a></li><li><a href='#82-networks'><span>8.2 Networks</span></a></li></ul></li><li><p><a href='#9-progress-summary'><span>9 Progress Summary</span></a></p></li><li><p><a href='#10-next-steps'><span>10 Next steps</span></a></p></li></ul><p><a href='http://ecotrust-canada.github.io/markdown-toc/'><span>Table of contents generated with markdown-toc</span></a></p><h2><a name="1-project-details" class="md-header-anchor"></a><span>1 Project Details</span></h2><figure><table><thead><tr><th>&nbsp;</th><th>&nbsp;</th></tr></thead><tbody><tr><td><span>Title</span></td><td><span>Artificial intelligence for reconstruction and super-resolution of chemical tomography</span></td></tr><tr><td><span>Funding reference</span></td><td><span>AI3SD-FundingCall2_017</span></td></tr><tr><td><span>Lead institution</span></td><td><span>STFC</span></td></tr><tr><td><span>Project dates</span></td><td><span>01/05/2020 - 31/10/2020</span></td></tr><tr><td><span>Website</span></td><td><a href='https://superres-tomo.readthedocs.io/en/latest/about.html'><span>Project</span></a><span> </span><a href='https://www.scd.stfc.ac.uk/Pages/Scientific-Machine-Learning.aspx'><span>SciML STFC</span></a><span> </span><a href='www.finden.co.uk'><span>Finden</span></a></td></tr><tr><td><span>Keywords</span></td><td><span>Chemical tomography, machine learning, CNNs, GANs</span></td></tr></tbody></table></figure><h2><a name="2-project-team" class="md-header-anchor"></a><span>2 Project Team</span></h2><h3><a name="21-principal-investigator" class="md-header-anchor"></a><span>2.1 Principal investigator</span></h3><p><strong><span>Name and Title:</span></strong><span> Dr Keith Butler (Data Scientist)</span>
<strong><span>Association:</span></strong><span> STFC, Scientific Computing Department</span>
<strong><span>Work Email:</span></strong><span> keith.butler@stfc.ac.uk</span>
<strong><span>Work Phone:</span></strong><span> 01925 606541</span></p><h3><a name="22-co-investigators" class="md-header-anchor"></a><span>2.2 Co-investigators</span></h3><p><strong><span>Name and Title:</span></strong><span> Mr Honyang Dong (PhD student)</span>
<strong><span>Association:</span></strong><span> University College London and Finden Ltd</span>
<strong><span>Work Email:</span></strong><span> hongyang.dong.18@ucl.ac.uk</span>
<strong><span>Work Phone:</span></strong><span> 01235 56 7397</span></p><p><strong><span>Name and Title:</span></strong><span> Dr Antony Vamvakeros (Research Scientist)</span>
<strong><span>Association:</span></strong><span> Finden Ltd</span>
<strong><span>Work Email:</span></strong><span> </span><a href='mailto:antony@finden.co.uk' target='_blank' class='url'>antony@finden.co.uk</a>
<strong><span>Work Phone:</span></strong><span> 01235 56 7397</span></p><p><strong><span>Name and Title:</span></strong><span> Dr Simon Jacques (Managing Director)</span>
<strong><span>Association:</span></strong><span> Finden Ltd</span>
<strong><span>Work Email:</span></strong><span> </span><a href='mailto:simon@finden.co.uk' target='_blank' class='url'>simon@finden.co.uk</a>
<strong><span>Work Phone:</span></strong><span> 01235 56 7397</span></p><h2><a name="3-publicity-summary" class="md-header-anchor"></a><span>3. Publicity summary</span></h2><p><span>X-ray scatter-based tomography allows unprecedented insight into the chemical and physical state of functional materials and devices. Such tomographies can be used as research tools but also offer the prospect of routine scanning for security and inspection systems and potential for medical scanning. However X-ray scatter tomogrpahy requires longer collection times and higher doses than conventional absorption tomography - in this project we will develop machine learning tools to fuse X-ray scatter and X-ray absorption tomogrpahy, providing the detail of the former with the efficiency of the latter.</span></p><p><span>In conventional X-ray tomography, the images that are obtained give maps of density within the object and the composing pixels contain single grey scale values. In scatter based tomography, each pixel instead contains spectrum or equivalent chemical signal i.e. a 1D array (or higher) of numbers. An X-ray scatter tomography slice becomes a data cube with the two conventional spatial dimensions and a third spectral dimension. Such image data is termed hyperspectral.   </span></p><p><img src="https://superres-tomo.readthedocs.io/en/latest/_images/cnn-reconstruct.png" referrerpolicy="no-referrer" alt="alternate text"><em><span>Figure 1 A neural network converts a sinogram (left), into a real-space image (right).</span></em></p><p><span>Whilst hyperspectral tomography can match or even exceed the resolution offered by conventional X-ray absorption contrast tomography, the latter is more highly optimised and offers modalities that can generate images in a fraction of the time and dose to image the same volume. In practice it is often the case that hyperspectral tomography resolution is sacrificed to accelerate collection time. This project aims to exploit machine learning approaches to marry hyperspectral chemical tomography with conventional X-ray absorption tomography to achieve chemical images with the rich information of the former in combination with the resolution and speed of collection of the latter.</span></p><h2><a name="4-executive-summary" class="md-header-anchor"></a><span>4 Executive summary</span></h2><p><span>X-ray scatter based scatter tomography are extremely powerful non-destructive analytical techniques that can provide insight into the chemical and physical states of functional materials and devices even under operating conditions. These approaches have potential for applications beyond their current use as research tools, but to date this has not been realised in large due to the long collection times and high dose rates associated with measurement. The aim in this project is to facilitate shorter data collections that can still yield high-resolution images by using machine learning approaches to achieve super-resolution. To achieve this we will use the information from the large number of data points within the hyperspectral X-ray scatter dataset and combine with the traditional conventional X-ray absorption signal which can be easily and quickly measured. This project will focus on (1) developing and applying novel AI-based methods for chemical image (volume) reconstruction and (2) enhancing the spatial resolution of the chemical images. </span></p><p><span>We will be applying the approaches to X-ray diffraction computed tomography technique. The state-of-the-art with this method, can yield large 3D volumes, containing many hundreds of thousands or even millions of diffraction patterns. These are extremely challenging and time consuming to processes and analyse. The continuing development in instrumentation means that this big data problem is only increasing and indeed this problem becomes even worse when higher resolution chemical images are obtained.</span></p><p><span>The new machine learning based algorithms that are being developed will automate or semi-automate the image reconstruction process while the image enhancement will allow the collection of less data to achieve the same resolution (both cases falling under the big data handling umbrella). These new reconstruction and image enhancement approaches will be immediately beneficial in terms of advancing these techniques and offering the prospect of translation beyond their use as research tools. Of even greater value will be the public availability of the baseline model and a benchmark dataset, which can stimulate research across the community. </span></p><h2><a name="5-aims-and-objectives" class="md-header-anchor"></a><span>5 Aims and objectives</span></h2><p><span>We aim to develop generally applicable, freely available tools for using machine learning in X-ray computed tomography. Specifically we will develop:</span></p><ul><li><span>Open datasets for testing tomographic reconstruction and super-resolution approaches</span></li><li><span>A benchmark baseline for tomographic reconstruction with neural networks</span></li><li><span>A benchmark baseline for super-resolution enhancement of XRD-CT images using neural networks</span></li><li><span>A documented, open-source repository for these tools and data</span></li></ul><p><span>Our overarching aim is to accelerate machine learning application in XRD-CT by providing examples, baselines and test datasets. </span></p><h2><a name="6-methodology" class="md-header-anchor"></a><span>6 Methodology</span></h2><h3><a name="61-scientific-methodology" class="md-header-anchor"></a><span>6.1 Scientific methodology</span></h3><p><span>The first part of the project focused on creating training libraries for the neural networks. To avoid bias, we have created libraries based on: (1) synthetic data containing random shapes using the well-known </span><code>scikit-image</code><span> Python package, (2) the DIV2K dataset: DIVerse 2K resolution high quality images as used for the challenges @ NTIRE (CVPR 2017 and CVPR 2018) and @ PIRM (ECCV 2018) and (3) images reconstructed from previously acquired micro-CT and XRD-CT experimental datasets. For the experimental tomographic datasets, the sinograms were first centered, scaled (i.e. assuming equal summed intensity per tomographic angle), background was subtracted (e.g. air scattering for the XRD-CT data) and then the images were reconstructing with the filtered back projection algorithm setting all negative values to zero. Specifically for the XRD-CT datasets, appropriate filters (i.e. trimmed mean filter) were applied to the raw 2D diffraction images during radial integration to avoid the formation of hotspots in the sinograms. These processed images are considered to be the ground truth for these libraries. Where needed, these images were rescaled to lower resolution using bilinear interpolation and artificial sinograms were created using the </span><code>astra toolbox</code><span> in Python. The performance of the reconstruction models is evaluated by comparing the reconstruction results of these sinograms with the ones obtained using the filtered back projection (i.e. reconstruction of synthetic sinograms). Similarly, for the super-resolution, the performance of the CNNs will be evaluated using the aforementioned three libraries.</span></p><h3><a name="62-ai-methodology" class="md-header-anchor"></a><span>6.2 AI methodology</span></h3><p><span>We used a mixed architecture for CNN reconstruction. It starts with four 2D convolutional layers whose strides are equal to 2, followed by four fully connected layers. Then the 1D output from the last fully connected layer is transformed back to a 2D image and then sent to the next three 2D convolutional layers whose strides are equal to 1. For fully connected layers, there are 1000 nodes inside each of the first three of them, and the last fully connected layer has the size equal to the number of pixels of the reconstructed image. Besides, there is also one dropout layer after each fully connected layer to avoid overfitting. </span></p><p><span>The accuracy and quality of the reconstructed images can be improved by increasing the number of nodes in the first three dense layers, but those numbers are significantly restricted by the computing resources. Increasing the number of nodes inside each layer can lead to a dramatic increase of the trainable weights, which makes the model harder to fit. Therefore, we added four more convolutional layers afterwards. The convolutional layers have significantly fewer weights than dense layers. They are used to take out the best-fitting features of the images and refine the reconstruction.</span></p><p><span>For the super-resolution, we are planning first to implement and evaluate the performance of the </span><code>EDSR</code><span>, </span><code>WDSR</code><span> and </span><code>SRGAN</code><span> neural networks using the aforementioned three libraries containing different types of image data. The next step will involve exploring their performance for upscaling micro-CT and for the first time, XRD-CT data. Finally, new architectures will be explored specifically for the XRD-CT data using a dual input of low resolution XRD-CT images and a high resolution micro-CT image acquired at the same position (i.e. XRD-CT and micro-CT corresponding at the same sample cross-section).</span></p><h2><a name="7-interim-results" class="md-header-anchor"></a><span>7 Interim results</span></h2><h3><a name="71-libraries" class="md-header-anchor"></a><span>7.1 Libraries</span></h3><p><span>We have made a publicly available set of libraries for training and testing tomographic reconstruction and X-ray image super-resolution techniques. There is one library of 180,000 sinogram image pairs for reconstruction. As described above there are also several libraries of experimental micro-CT and XRD-CT data at different resolutions (for the same data), which can be used to train and test super-resolution methods. The links and details of the libraries can be found on the project documentation pages:  </span><a href='https://superres-tomo.readthedocs.io/en/latest/benchmark_data.html' target='_blank' class='url'>https://superres-tomo.readthedocs.io/en/latest/benchmark_data.html</a><span>.</span></p><h3><a name="72-cnn-tomographic-reconstruction" class="md-header-anchor"></a><span>7.2 CNN tomographic reconstruction</span></h3><p><span>For the purpose of this project, we have decided to use the currently latest stable version of Tensorflow (v.2.2.0) in python for the development and testing of the neural networks. We designed, implemented and evaluated a large number of CNN architectures, exploring also the impact of various hyperparameters (e.g. learning rate) and loss functions. From the various performance tests, we found the </span><code>cnn_reconstruct</code><span> discussed in the 6.2 section as one of the most promising ones, especially due to its ability for upscaling (i.e. it can handle relatively large images while maintaining a number of parameters in the order of 107 - 108). The learning rate was set to 0.00025 and the root mean squared was used as the loss function. We created a library using a combination of experimental XRD-CT datasets using catalyst particles consisting of 8,000 pairs of sinograms-images. Some examples are shown in Figure 2 where the original image and the ones obtained from the reconstruction using the filtered back projection algorithm and the cnn respectively are presented.</span></p><p><img src="https://lh3.googleusercontent.com/wTQCN8c8fjpY-3mmwWDFFHhv90Pc-cZCreuv2yIs7wX4k2ZBkdg7nAYNxcn9ERjnpWkP1uAzBc9uTSe2Oj2h4uSrPEZ5FCBah8RY3r3goGyMKFsIYkgE0fyxRC1ztqI3YLv0zkQ" referrerpolicy="no-referrer" alt="img"><em><span>Figure 2 Performance of the reconstruction CNN and its comparison with the results obtained with the filtered back projection algorithm using a library containing XRD-CT sinograms-images.</span></em></p><p><span>It can be seen here that the performance of the </span><code>cnn_reconstruct</code><span> is superior to the conventional filtered back projection algorithm as it can correctly reconstruct the shape and intensity of the catalyst particles while at the same time suppressing the background noise. However, when the same CNN was tested using the library containing random shapes (55,000 pairs of sinograms-images), the performance was worse. These results are presented in Figure 3. </span></p><p><img src="https://lh4.googleusercontent.com/3C59kjTjkwXUP-uY9ntVLNizfEYS8S9OW2VmzQ0WJHPBb7n22ymoI4PWCOtl4rCTsrm9ExiwwTWsA3t-0NgL342HMyPgF33ds6n-SvO2oab7Oop75KYF106TqfQNivXxZ-yaU1w" referrerpolicy="no-referrer" alt="img"><em><span>Figure 3 Performance of the reconstruction CNN and its comparison with the results obtained with the filtered back projection algorithm using a library containing synthetic sinograms-images of random shapes.</span></em></p><p><span>It can be seen that the filtered back projection algorithm can retain the sharp edges of the shapes and their overall shape while the reconstruction CNN fails to do so. The problem here arises from the training data as the various shape images can vary significantly in content while there is a high degree of correlation between the XRD-CT images (i.e. the XRD-CT images present in each XRD-CT dataset). </span></p><p><span>These results are very important as they illustrate the strong dependence of the reconstruction CNN on the training data and the difficulty in creating a reconstruction CNN that can handle very different data (i.e. images that are not well correlated with the training data). </span><strong><span>This major issue is rarely discussed in literature and the current results from this project show that there should be more discussion on the impact and nature of training data used in supervised learning reconstruction CNNs.</span></strong><span> As an example, in literature one can often encounter CNN&#39;s used in medical imaging (reconstruction, denoising etc) claimed to exhibit superior performance compared to conventional methods but the CNN&#39;s have been trained using only training libraries containing medical CT for a body part/organ. We are currently working on an unsupervised machine learning approach for image reconstruction using a Generative Adversarial Network (GAN) inspired by the recently published  </span><code>GANrec</code><span> which should eliminate this bias caused by the nature of the training data in supervised learning approaches.</span></p><h3><a name="73-msdn-for-reconstructionsuper-resolution" class="md-header-anchor"></a><span>7.3 MSDN for reconstruction/super-resolution</span></h3><p><span>Initially in the proposed work plan we had intended to test the recently developed Mixed Scale Dense Networks (</span><a href='https://doi.org/10.1073/pnas.1715832114' target='_blank' class='url'>https://doi.org/10.1073/pnas.1715832114</a><span>) for both image reconstruction and super-resolution. However when we further explored the architecture and its implementation, it has become apparent that the MSDN is restricted to inputs and outputs with exactly the same dimensions. This makes reconstruction impossible and limits the application for super-resolution. As a result we have decided to concentrate our efforts on Convolutional Neural Networks and Generative Adversarial Networks. </span></p><h2><a name="8-outputs" class="md-header-anchor"></a><span>8 Outputs</span></h2><p><span>We have collected the outputs from the project in a repository, hosted on GitHub. We have also been building documentation to describe the outputs of the project, as well as an API, with extensive tutorials to allow others to use the tools and the data resulting from the project. The documentation can be found at: </span><a href='https://superres-tomo.readthedocs.io/en/latest/about.html' target='_blank' class='url'>https://superres-tomo.readthedocs.io/en/latest/about.html</a><span> and the code repository at </span><a href='https://github.com/keeeto/super-resolution-ml' target='_blank' class='url'>https://github.com/keeeto/super-resolution-ml</a></p><h3><a name="81-libraries" class="md-header-anchor"></a><span>8.1 Libraries</span></h3><p><span>The data libraries are stored in </span><code>hdf5</code><span> format, to facilitate easy and efficient use in machine learning projects. The library locations and details are documented in the project docs at: </span><a href='https://superres-tomo.readthedocs.io/en/latest/benchmark_data.html' target='_blank' class='url'>https://superres-tomo.readthedocs.io/en/latest/benchmark_data.html</a></p><h3><a name="82-networks" class="md-header-anchor"></a><span>8.2 Networks</span></h3><p><span>So far we have developed networks for image reconstruction, segmentation and denoising. We have packaged these together in a consistent fashion in our GitHub repository (</span><a href='https://github.com/keeeto/super-resolution-ml' target='_blank' class='url'>https://github.com/keeeto/super-resolution-ml</a><span>) to ensure that the code can be used by others. We have also written extensive tutorials to demonstrate how to train and apply the networks for these different tasks</span></p><ul><li><p><span>Reconstructions</span></p><ul><li><span>With a CNN: </span><a href='https://superres-tomo.readthedocs.io/en/latest/tutorials.html#reconstruction-with-a-cnn' target='_blank' class='url'>https://superres-tomo.readthedocs.io/en/latest/tutorials.html#reconstruction-with-a-cnn</a></li><li><span>With a dense network: </span><a href='https://superres-tomo.readthedocs.io/en/latest/tutorials.html#reconstruction-with-a-dense-network' target='_blank' class='url'>https://superres-tomo.readthedocs.io/en/latest/tutorials.html#reconstruction-with-a-dense-network</a></li><li><span>With an Automap network:</span></li></ul></li><li><p><span>Segmentation </span><a href='https://superres-tomo.readthedocs.io/en/latest/tutorials.html#segmentation-of-x-ray-images' target='_blank' class='url'>https://superres-tomo.readthedocs.io/en/latest/tutorials.html#segmentation-of-x-ray-images</a></p></li><li><p><span>Denoising  </span><a href='https://superres-tomo.readthedocs.io/en/latest/tutorials.html#denoising-of-x-ray-images' target='_blank' class='url'>https://superres-tomo.readthedocs.io/en/latest/tutorials.html#denoising-of-x-ray-images</a></p></li></ul><h2><a name="9-progress-summary" class="md-header-anchor"></a><span>9 Progress Summary</span></h2><p><span>The project is advancing well.  We have completed the planned work in WP0, WP1 and WP2  by the end of the reporting period and made progress on some upcoming deliverables for WP3 (Table 1). Simulated and real micro CT and XRD-CT libraries have been constructed.  XRD-CT data can be simulated quickly and libraries have been built that contain large numbers of simulated diffraction patterning (with and without addition of noise) containing a  controlled spread of the embedded physico-chemical information. Details of exemplar datasets can be found in section 8 above. WP2 has been concerned with developing code for image reconstruction.  This was successfully achieved using CNN’s; the MSDN was abandoned for this task.  The work on the GAN’s has started and these approaches are giving good initial results.</span></p><figure><table><thead><tr><th><span>WP</span></th><th><span>Task / deliverable</span></th><th><span>Start</span></th><th><span>Days</span></th><th><span>End</span></th><th><span>Status</span></th><th><span>Comments</span></th></tr></thead><tbody><tr><td><span>0</span></td><td><span>Choice of AI strategies and algorithms</span></td><td><span>01/05</span></td><td><span>15</span></td><td><span>11/05</span></td><td><span>100%</span></td><td>&nbsp;</td></tr><tr><td><span>1</span></td><td><span>Micro-CT and XRD-CT libraries</span></td><td><span>01/05</span></td><td><span>40</span></td><td><span>15/06</span></td><td><span>100%</span></td><td>&nbsp;</td></tr><tr><td><span>2</span></td><td><span>CNN for image reconstruction</span></td><td><span>15/05</span></td><td><span>40</span></td><td><span>29/06</span></td><td><span>100%</span></td><td>&nbsp;</td></tr><tr><td><span>2</span></td><td><span>MSDN for image reconstruction</span></td><td><span>15/06</span></td><td><span>25</span></td><td><span>13/06</span></td><td><span>100%</span></td><td><span>approach failed</span></td></tr><tr><td><span>2</span></td><td><span>GAN for image reconstruction</span></td><td><span>10/07</span></td><td><span>22</span></td><td><span>10/07</span></td><td><span>50%</span></td><td>&nbsp;</td></tr><tr><td><span>3</span></td><td><span>CNN for super-resolution</span></td><td><span>15/08</span></td><td><span>50</span></td><td>&nbsp;</td><td><span>50%</span></td><td>&nbsp;</td></tr><tr><td><span>3</span></td><td><span>MSDN for super-resolution</span></td><td><span>01/09</span></td><td><span>0</span></td><td>&nbsp;</td><td><span>-</span></td><td><span>not viable route</span></td></tr><tr><td><span>3</span></td><td><span>GAN for super-resolution</span></td><td><span>20/09</span></td><td><span>40</span></td><td>&nbsp;</td><td><span>0%</span></td><td>&nbsp;</td></tr><tr><td><span>0</span></td><td><span>Final report and paper(s)</span></td><td><span>16/10</span></td><td><span>15</span></td><td>&nbsp;</td><td><span>0%</span></td><td>&nbsp;</td></tr></tbody></table></figure><p><em><span>NB The effort in WP3 has been updated to reflect the change in the plan due to the failure of the MSDN route ( highlighted in orange).</span></em></p><h2><a name="10-next-steps" class="md-header-anchor"></a><span>10 Next steps</span></h2><p><span>The next steps in the project are to complete the GAN development. The work on super-resolution in WP3 will begin shortly. The failure of the MSDN route will give us more time to work on the CNN and GAN approaches. Accordingly, we have modified our projected effort for these tasks.</span></p><p><span>We are going to produce further public datasets for image reconstruction.</span></p><p><span>We are also writing automated testing for the GitHub repository to ensure the stability of the code. We will continue to publish, document and test the new models and methods that we develop from the project. </span></p><p><span>We hope to write the results of this project into some publications, potentially including publishing the software in the Journal of Open Source Software.</span></p></div>
</body>
</html>